{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = ['python', 'парсинг']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://habr.com/ru/search/'\n",
    "params = {\n",
    "    'q': KEYWORDS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(URL, params)\n",
    "soup = BeautifulSoup(req.text)\n",
    "posts = soup.find_all(class_ = ['post__time', 'post__title_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "heading = []\n",
    "link = []\n",
    "for post in posts:\n",
    "    if 'post__time' in post.attrs['class']:\n",
    "        date.append(post.string)\n",
    "    elif 'post__title_link' in post.attrs['class']:\n",
    "        # Т.к. заголовок возвращается в виде списка из текста заголовка и тега поискового запроса, при помощи .contents получаем\n",
    "        # значение тега и остальной текст заголовка в виде списка слов, помещаем их в heading_2. Затем преобразуем heading_2 в стороку,  \n",
    "        # добавляем строку в список heading, а heading_2 очищаем.\n",
    "        heading_2 = []\n",
    "        for word in post.contents:\n",
    "            heading_2.append(word.string)\n",
    "        heading.append(''.join(heading_2))\n",
    "        heading_2 = []\n",
    "        link.append(post.attrs['href'])\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Дата</th>\n",
       "      <th>Заголовок</th>\n",
       "      <th>Ссылка</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 апреля 2019 в 15:21</td>\n",
       "      <td>Правда про парсинг сайтов, или «все интернет-м...</td>\n",
       "      <td>https://habr.com/ru/post/446488/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 марта 2021 в 15:01</td>\n",
       "      <td>Парсинг — это законно?</td>\n",
       "      <td>https://habr.com/ru/post/545818/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 октября 2017 в 14:04</td>\n",
       "      <td>Парсинг сайтов: как с  ...</td>\n",
       "      <td>https://habr.com/ru/post/340302/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7 мая 2019 в 14:51</td>\n",
       "      <td>Парсинг сайтов — а это вообще легально в России?</td>\n",
       "      <td>https://habr.com/ru/post/450834/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 октября 2020 в 17:17</td>\n",
       "      <td>Как выбрать решение для парсинга сайтов: класс...</td>\n",
       "      <td>https://habr.com/ru/post/521646/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1 декабря 2016 в 12:46</td>\n",
       "      <td>Пример использования Product API от Fetchee дл...</td>\n",
       "      <td>https://habr.com/ru/company/fetchee/blog/316558/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 марта 2016 в 16:39</td>\n",
       "      <td>Теория и практика парсинга исходников с помощь...</td>\n",
       "      <td>https://habr.com/ru/company/pt/blog/210772/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18 июня 2018 в 23:42</td>\n",
       "      <td>Парсинг и работа с Codable в Swift 4</td>\n",
       "      <td>https://habr.com/ru/post/414221/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28 августа 2019 в 14:12</td>\n",
       "      <td>Парсинг и анализ семантики для SEO: 5 бесплатн...</td>\n",
       "      <td>https://habr.com/ru/company/click/blog/465277/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14 июня 2011 в 19:24</td>\n",
       "      <td>Парсинг на Pуthon. Как собрать архив Голубятен</td>\n",
       "      <td>https://habr.com/ru/post/121815/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6 марта 2015 в 17:40</td>\n",
       "      <td>Веб-парсинг на Ruby</td>\n",
       "      <td>https://habr.com/ru/post/252379/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18 марта 2015 в 17:52</td>\n",
       "      <td>Продвинутый парсинг веб-сайтов с Mechanize</td>\n",
       "      <td>https://habr.com/ru/post/253439/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23 марта 2019 в 20:36</td>\n",
       "      <td>Искусство парсинга 2 или транслитерация собств...</td>\n",
       "      <td>https://habr.com/ru/post/444876/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23 июля 2020 в 15:17</td>\n",
       "      <td>Как проанализировать рынок фотостудий с помощь...</td>\n",
       "      <td>https://habr.com/ru/post/504900/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26 февраля 2018 в 10:28</td>\n",
       "      <td>Парсинг telegram каналов для агрегатора контен...</td>\n",
       "      <td>https://habr.com/ru/post/349942/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8 июня 2020 в 17:46</td>\n",
       "      <td>Парсинг YouTube, включая подгружаемые данные, ...</td>\n",
       "      <td>https://habr.com/ru/post/505888/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19 июля 2015 в 15:27</td>\n",
       "      <td>Использование morph.io для веб-парсинга</td>\n",
       "      <td>https://habr.com/ru/post/262991/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14 февраля 2013 в 14:46</td>\n",
       "      <td>Парсинг сайтов-магазинов. Личный опыт и немног...</td>\n",
       "      <td>https://habr.com/ru/post/169409/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25 мая 2009 в 21:15</td>\n",
       "      <td>Еще о парсинге на Prolog'е</td>\n",
       "      <td>https://habr.com/ru/post/60430/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2 сентября 2011 в 14:37</td>\n",
       "      <td>Grab — python библиотека для парсинга сайтов</td>\n",
       "      <td>https://habr.com/ru/post/127584/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Дата  \\\n",
       "0     3 апреля 2019 в 15:21   \n",
       "1     10 марта 2021 в 15:01   \n",
       "2   17 октября 2017 в 14:04   \n",
       "3        7 мая 2019 в 14:51   \n",
       "4    1 октября 2020 в 17:17   \n",
       "5    1 декабря 2016 в 12:46   \n",
       "6      3 марта 2016 в 16:39   \n",
       "7      18 июня 2018 в 23:42   \n",
       "8   28 августа 2019 в 14:12   \n",
       "9      14 июня 2011 в 19:24   \n",
       "10     6 марта 2015 в 17:40   \n",
       "11    18 марта 2015 в 17:52   \n",
       "12    23 марта 2019 в 20:36   \n",
       "13     23 июля 2020 в 15:17   \n",
       "14  26 февраля 2018 в 10:28   \n",
       "15      8 июня 2020 в 17:46   \n",
       "16     19 июля 2015 в 15:27   \n",
       "17  14 февраля 2013 в 14:46   \n",
       "18      25 мая 2009 в 21:15   \n",
       "19  2 сентября 2011 в 14:37   \n",
       "\n",
       "                                            Заголовок  \\\n",
       "0   Правда про парсинг сайтов, или «все интернет-м...   \n",
       "1                              Парсинг — это законно?   \n",
       "2                          Парсинг сайтов: как с  ...   \n",
       "3    Парсинг сайтов — а это вообще легально в России?   \n",
       "4   Как выбрать решение для парсинга сайтов: класс...   \n",
       "5   Пример использования Product API от Fetchee дл...   \n",
       "6   Теория и практика парсинга исходников с помощь...   \n",
       "7                Парсинг и работа с Codable в Swift 4   \n",
       "8   Парсинг и анализ семантики для SEO: 5 бесплатн...   \n",
       "9      Парсинг на Pуthon. Как собрать архив Голубятен   \n",
       "10                                Веб-парсинг на Ruby   \n",
       "11         Продвинутый парсинг веб-сайтов с Mechanize   \n",
       "12  Искусство парсинга 2 или транслитерация собств...   \n",
       "13  Как проанализировать рынок фотостудий с помощь...   \n",
       "14  Парсинг telegram каналов для агрегатора контен...   \n",
       "15  Парсинг YouTube, включая подгружаемые данные, ...   \n",
       "16            Использование morph.io для веб-парсинга   \n",
       "17  Парсинг сайтов-магазинов. Личный опыт и немног...   \n",
       "18                         Еще о парсинге на Prolog'е   \n",
       "19       Grab — python библиотека для парсинга сайтов   \n",
       "\n",
       "                                              Ссылка  \n",
       "0                   https://habr.com/ru/post/446488/  \n",
       "1                   https://habr.com/ru/post/545818/  \n",
       "2                   https://habr.com/ru/post/340302/  \n",
       "3                   https://habr.com/ru/post/450834/  \n",
       "4                   https://habr.com/ru/post/521646/  \n",
       "5   https://habr.com/ru/company/fetchee/blog/316558/  \n",
       "6        https://habr.com/ru/company/pt/blog/210772/  \n",
       "7                   https://habr.com/ru/post/414221/  \n",
       "8     https://habr.com/ru/company/click/blog/465277/  \n",
       "9                   https://habr.com/ru/post/121815/  \n",
       "10                  https://habr.com/ru/post/252379/  \n",
       "11                  https://habr.com/ru/post/253439/  \n",
       "12                  https://habr.com/ru/post/444876/  \n",
       "13                  https://habr.com/ru/post/504900/  \n",
       "14                  https://habr.com/ru/post/349942/  \n",
       "15                  https://habr.com/ru/post/505888/  \n",
       "16                  https://habr.com/ru/post/262991/  \n",
       "17                  https://habr.com/ru/post/169409/  \n",
       "18                   https://habr.com/ru/post/60430/  \n",
       "19                  https://habr.com/ru/post/127584/  "
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_habr = pd.DataFrame({'Дата': date, 'Заголовок': heading, 'Ссылка': link})\n",
    "df_habr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = ['xxx@x.ru', 'yyy@y.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_2 = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "params_2 = {\n",
    "    'emailAddresses':EMAIL\n",
    "}\n",
    "headers = {\n",
    "    'Vaar-Version': '0',\n",
    "    'Vaar-Header-App-Product-Name': 'hackcheck-web-avast',\n",
    "    'Vaar-Header-App-Build-Version': '1.0.0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_2 = requests.post(URL_2, json={'emailAddresses': EMAIL}, headers=headers)\n",
    "req_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = req_2.json()['data']\n",
    "source_of_leak = []\n",
    "emails = []\n",
    "breachId = []\n",
    "for row in data:\n",
    "    # Собираем источники утечки\n",
    "    source_of_leak.append(row)\n",
    "    for email in data[row].keys():\n",
    "        # собираем почты\n",
    "        emails.append(email)\n",
    "        for brId in data[row][email]:\n",
    "            #Собираем ID утечек\n",
    "            breachId.append(brId['breachId'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaches = req_2.json()['breaches']\n",
    "description_of_leak = []\n",
    "publish_date = []\n",
    "for i in breachId:\n",
    "    # собираем описания утечек по ID утечек\n",
    "    description_of_leak.append(breaches[str(i)]['description'])\n",
    "    # собираем даты утечек по ID утечек\n",
    "    publish_date.append(breaches[str(i)]['publishDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Почта</th>\n",
       "      <th>Дата утечки</th>\n",
       "      <th>Источник утечки</th>\n",
       "      <th>Описание утечки</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2017-02-14T00:00:00Z</td>\n",
       "      <td>parapa.mail.ru</td>\n",
       "      <td>In July and August 2016, two criminals execute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-03-28T00:00:00Z</td>\n",
       "      <td>verifications.io</td>\n",
       "      <td>Big data e-mail verification platform verifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2016-10-21T00:00:00Z</td>\n",
       "      <td>adobe.com</td>\n",
       "      <td>In October of 2013, criminals penetrated Adobe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2017-01-31T00:00:00Z</td>\n",
       "      <td>cdprojektred.com</td>\n",
       "      <td>In March 2016, CDProjektRed.com.com's forum da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-12-01T00:00:00Z</td>\n",
       "      <td>creocommunity.com</td>\n",
       "      <td>At an unconfirmed date, Creo Community's user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-07-11T00:00:00Z</td>\n",
       "      <td>medicaresupplement.com</td>\n",
       "      <td>In May 2019, a security researcher discovered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2020-01-03T00:00:00Z</td>\n",
       "      <td>azcentral.com</td>\n",
       "      <td>At an unconfirmed date, online Arizona newspap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2020-11-19T00:00:00Z</td>\n",
       "      <td>123rf.com</td>\n",
       "      <td>In March 2020, the stock image agency 123RF wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2016-10-21T00:00:00Z</td>\n",
       "      <td>linkedin.com</td>\n",
       "      <td>In 2012, online professional networking platfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2016-10-24T00:00:00Z</td>\n",
       "      <td>dropbox.com</td>\n",
       "      <td>Cloud storage company Dropbox suffered a major...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2020-05-28T00:00:00Z</td>\n",
       "      <td>wishbone.io</td>\n",
       "      <td>In January 2020, the online poll website Wishb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-03-24T00:00:00Z</td>\n",
       "      <td>youku.com</td>\n",
       "      <td>Youku is a large Chinese video content company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-02-21T00:00:00Z</td>\n",
       "      <td>www.dangdang.com</td>\n",
       "      <td>This is a list of email addresses only, and as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-11-04T00:00:00Z</td>\n",
       "      <td>myheritage.com</td>\n",
       "      <td>In October 2017, a customer database belonging...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-03-01T00:00:00Z</td>\n",
       "      <td>rayli.com.cn</td>\n",
       "      <td>On an unconfirmed date, Chinese gossip site Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-06-13T00:00:00Z</td>\n",
       "      <td>canva.com</td>\n",
       "      <td>In May 2019, graphic-design site Canva's datab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2017-03-15T00:00:00Z</td>\n",
       "      <td>globalreach.eu</td>\n",
       "      <td>In 2016, Global Reach Technology's database wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2017-02-14T00:00:00Z</td>\n",
       "      <td>cfire.mail.ru</td>\n",
       "      <td>In July and August of 2016, two criminals carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2016-10-29T00:00:00Z</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>Popular Russian social networking platform VKo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2021-02-11T00:00:00Z</td>\n",
       "      <td>forums.vkmonline.com</td>\n",
       "      <td>At an unconfirmed date, the Russian-language m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2018-02-18T00:00:00Z</td>\n",
       "      <td>netlog.com</td>\n",
       "      <td>Netlog (formerly known as Facebox and Bingbox)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>xxx@x.ru</td>\n",
       "      <td>2016-10-23T00:00:00Z</td>\n",
       "      <td>imesh.com</td>\n",
       "      <td>In June 2016, a cache of over 51 million user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yyy@y.com</td>\n",
       "      <td>2019-10-17T00:00:00Z</td>\n",
       "      <td>zynga.com</td>\n",
       "      <td>In September 2019, the game developer Zynga wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Почта           Дата утечки         Источник утечки  \\\n",
       "0    xxx@x.ru  2017-02-14T00:00:00Z          parapa.mail.ru   \n",
       "1   yyy@y.com  2019-03-28T00:00:00Z        verifications.io   \n",
       "2    xxx@x.ru  2016-10-21T00:00:00Z               adobe.com   \n",
       "3    xxx@x.ru  2017-01-31T00:00:00Z        cdprojektred.com   \n",
       "4   yyy@y.com  2017-12-01T00:00:00Z       creocommunity.com   \n",
       "5   yyy@y.com  2019-07-11T00:00:00Z  medicaresupplement.com   \n",
       "6   yyy@y.com  2020-01-03T00:00:00Z           azcentral.com   \n",
       "7   yyy@y.com  2020-11-19T00:00:00Z               123rf.com   \n",
       "8   yyy@y.com  2016-10-21T00:00:00Z            linkedin.com   \n",
       "9   yyy@y.com  2016-10-24T00:00:00Z             dropbox.com   \n",
       "10  yyy@y.com  2020-05-28T00:00:00Z             wishbone.io   \n",
       "11  yyy@y.com  2017-03-24T00:00:00Z               youku.com   \n",
       "12  yyy@y.com  2019-02-21T00:00:00Z        www.dangdang.com   \n",
       "13  yyy@y.com  2017-11-04T00:00:00Z          myheritage.com   \n",
       "14  yyy@y.com  2017-03-01T00:00:00Z            rayli.com.cn   \n",
       "15  yyy@y.com  2019-06-13T00:00:00Z               canva.com   \n",
       "16  yyy@y.com  2017-03-15T00:00:00Z          globalreach.eu   \n",
       "17   xxx@x.ru  2017-02-14T00:00:00Z           cfire.mail.ru   \n",
       "18   xxx@x.ru  2016-10-29T00:00:00Z                  vk.com   \n",
       "19  yyy@y.com  2021-02-11T00:00:00Z    forums.vkmonline.com   \n",
       "20  yyy@y.com  2018-02-18T00:00:00Z              netlog.com   \n",
       "21   xxx@x.ru  2016-10-23T00:00:00Z               imesh.com   \n",
       "22  yyy@y.com  2019-10-17T00:00:00Z               zynga.com   \n",
       "\n",
       "                                      Описание утечки  \n",
       "0   In July and August 2016, two criminals execute...  \n",
       "1   Big data e-mail verification platform verifica...  \n",
       "2   In October of 2013, criminals penetrated Adobe...  \n",
       "3   In March 2016, CDProjektRed.com.com's forum da...  \n",
       "4   At an unconfirmed date, Creo Community's user ...  \n",
       "5   In May 2019, a security researcher discovered ...  \n",
       "6   At an unconfirmed date, online Arizona newspap...  \n",
       "7   In March 2020, the stock image agency 123RF wa...  \n",
       "8   In 2012, online professional networking platfo...  \n",
       "9   Cloud storage company Dropbox suffered a major...  \n",
       "10  In January 2020, the online poll website Wishb...  \n",
       "11  Youku is a large Chinese video content company...  \n",
       "12  This is a list of email addresses only, and as...  \n",
       "13  In October 2017, a customer database belonging...  \n",
       "14  On an unconfirmed date, Chinese gossip site Ra...  \n",
       "15  In May 2019, graphic-design site Canva's datab...  \n",
       "16  In 2016, Global Reach Technology's database wa...  \n",
       "17  In July and August of 2016, two criminals carr...  \n",
       "18  Popular Russian social networking platform VKo...  \n",
       "19  At an unconfirmed date, the Russian-language m...  \n",
       "20  Netlog (formerly known as Facebox and Bingbox)...  \n",
       "21  In June 2016, a cache of over 51 million user ...  \n",
       "22  In September 2019, the game developer Zynga wa...  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avast = pd.DataFrame({'Почта': emails, 'Дата утечки': publish_date, 'Источник утечки': source_of_leak, 'Описание утечки': description_of_leak})\n",
    "df_avast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
